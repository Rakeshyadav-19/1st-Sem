{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "X10kIrIBhWLi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n"
      ],
      "metadata": {
        "id": "0Y1Lc_cphZ59"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values to be between 0 and 1\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n"
      ],
      "metadata": {
        "id": "3xhqcmW2hp_f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to categorical format (one-hot encoding)\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "wYAk2D4LhteT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Sequential()\n"
      ],
      "metadata": {
        "id": "rJxVjtRMhwIh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the 28x28 images to a 784-element array\n",
        "model.add(Flatten(input_shape=(28, 28)))\n"
      ],
      "metadata": {
        "id": "H35gmu2GhzvG",
        "outputId": "2e87669a-9e71-45ad-f69b-fb049dea08ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dense(128, activation='relu'))\n"
      ],
      "metadata": {
        "id": "FEcV0e6gh19u"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add another hidden layer with 64 neurons and ReLU activation\n",
        "model.add(Dense(64, activation='relu'))\n"
      ],
      "metadata": {
        "id": "9k6bQtVeh4xP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the output layer with 10 neurons (for 10 classes) and softmax activation\n",
        "model.add(Dense(10, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "mo5z-oinh7FW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "loss='categorical_crossentropy',\n",
        "metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "7SHmO50Uh9nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Flatten, Reshape\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "# Assuming 'model' is a Keras Sequential model\n",
        "model = keras.Sequential()\n",
        "\n",
        "\n",
        "# Add a convolutional layer to process the input images\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "# input_shape is set to match your x_train data\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())  # Flatten the output for the Dense layers\n",
        "\n",
        "\n",
        "# Add layers (Your existing code from ipython-input-31-4cd9f25476aa, ipython-input-32-4cd9f25476aa, ipython-input-33-4cd9f25476aa)\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model (Your existing code from ipython-input-0-4cd9f25476aa)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now you can fit the model (Your existing code from ipython-input-34-4cd9f25476aa)\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "44nnI6EviCLJ",
        "outputId": "dbaf76a7-3be5-4dce-82a5-4597be9c3bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 48ms/step - accuracy: 0.3838 - loss: 1.6846 - val_accuracy: 0.5888 - val_loss: 1.1582\n",
            "Epoch 2/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 48ms/step - accuracy: 0.6020 - loss: 1.1234 - val_accuracy: 0.6329 - val_loss: 1.0401\n",
            "Epoch 3/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 47ms/step - accuracy: 0.6670 - loss: 0.9466 - val_accuracy: 0.6655 - val_loss: 0.9641\n",
            "Epoch 4/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 47ms/step - accuracy: 0.7104 - loss: 0.8319 - val_accuracy: 0.6817 - val_loss: 0.9288\n",
            "Epoch 5/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 47ms/step - accuracy: 0.7398 - loss: 0.7392 - val_accuracy: 0.6973 - val_loss: 0.8843\n",
            "Epoch 6/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 47ms/step - accuracy: 0.7656 - loss: 0.6708 - val_accuracy: 0.6662 - val_loss: 1.0165\n",
            "Epoch 7/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 46ms/step - accuracy: 0.7926 - loss: 0.5934 - val_accuracy: 0.6858 - val_loss: 0.9445\n",
            "Epoch 8/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 47ms/step - accuracy: 0.8157 - loss: 0.5308 - val_accuracy: 0.6804 - val_loss: 1.0277\n",
            "Epoch 9/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 46ms/step - accuracy: 0.8312 - loss: 0.4775 - val_accuracy: 0.7007 - val_loss: 0.9900\n",
            "Epoch 10/10\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 52ms/step - accuracy: 0.8527 - loss: 0.4211 - val_accuracy: 0.6782 - val_loss: 1.0479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Preprocess the input data\n",
        "def preprocess_image(image):\n",
        "  \"\"\"Resizes and converts an image to grayscale.\n",
        "\n",
        "  Args:\n",
        "    image: A 3D tensor representing an RGB image.\n",
        "\n",
        "  Returns:\n",
        "    A 2D tensor representing a grayscale image.\n",
        "  \"\"\"\n",
        "  # Resize to match the input shape of the model\n",
        "  image = tf.image.resize(image, [32, 32])  # Resize to 32x32 to match the model's input shape\n",
        "  # Don't convert to grayscale as the model expects 3 channels\n",
        "  #image = tf.image.rgb_to_grayscale(image)\n",
        "  return image\n",
        "\n",
        "# Apply preprocessing to the first 5 images in x_test\n",
        "preprocessed_images = [preprocess_image(img) for img in x_test[:5]]\n",
        "\n",
        "# Convert the list of preprocessed images back to a NumPy array\n",
        "# if necessary (e.g., if your model expects a NumPy array input)\n",
        "preprocessed_images = tf.stack(preprocessed_images, axis=0).numpy()\n",
        "\n",
        "# Make predictions using the preprocessed images\n",
        "predictions = model.predict(preprocessed_images)"
      ],
      "metadata": {
        "id": "5uiglwZ6iEvp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76cbf65f-982c-414a-ef99-2ee4567d43bb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Print predicted classes\n",
        "print(\"Predicted classes:\", np.argmax(predictions, axis=1))\n",
        "print(\"True classes:\", np.argmax(y_test[:5], axis=1))\n"
      ],
      "metadata": {
        "id": "t4bqjMunirNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b9eb97e-9df2-4ac0-8c0b-dca5f4a90744"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes: [3 8 8 0 4]\n",
            "True classes: [3 8 8 0 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n"
      ],
      "metadata": {
        "id": "XIZUo3fBizkU"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random image from the test set\n",
        "random_index = random.randint(0, x_test.shape[0] - 1)\n",
        "random_image = x_test[random_index]\n",
        "true_label = np.argmax(y_test[random_index])\n"
      ],
      "metadata": {
        "id": "OpYaFO40i4zK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a random image from the test set\n",
        "random_index = random.randint(0, x_test.shape[0] - 1)\n",
        "random_image = x_test[random_index]\n",
        "true_label = np.argmax(y_test[random_index])\n",
        "\n",
        "# Preprocess the random image to match the model's expected input shape\n",
        "random_image = preprocess_image(random_image) # Call the preprocess function\n",
        "\n",
        "# Add a batch dimension (if the model requires it)\n",
        "random_image = tf.expand_dims(random_image, axis=0).numpy()\n",
        "\n",
        "# Make a prediction on the random image\n",
        "prediction = model.predict(random_image)\n",
        "predicted_label = np.argmax(prediction)"
      ],
      "metadata": {
        "id": "EElSBQl5i7ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b7f5a3-d2b0-4a4a-875a-8898cc00f4ea"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the image and prediction result\n",
        "plt.imshow(random_image.squeeze(), cmap='gray') # Squeeze the array to remove the batch dimension\n",
        "plt.title(f\"True Label: {true_label}, Predicted Label: {predicted_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iTo_4SwFjAKN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "01eca435-07dd-4d95-b7a0-52a37983908f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y4UZgmpmjDR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}